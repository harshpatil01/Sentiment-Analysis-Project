

# ðŸ“ Sentiment Analysis of Customer Reviews Using NLP & Machine Learning  

## ðŸ“Œ Overview  
This project leverages **Natural Language Processing (NLP)** and **Machine Learning** techniques to analyze and classify **Amazon fine food reviews** as **Positive or Negative**. The dataset contains over **568,454 customer reviews spanning 10+ years**.  
We compare multiple ML models to find the best-performing one and deploy the final model as a **Streamlit Web Application**.  

---

## ðŸ“Š Table of Contents  
- [Dataset Overview](#dataset-overview)  
- [Exploratory Data Analysis (EDA)](#exploratory-data-analysis-eda)  
- [Data Preprocessing](#data-preprocessing)  
- [Machine Learning Models](#machine-learning-models)  
- [Model Evaluation & Results](#model-evaluation--results)  
- [Deployment Using Streamlit](#deployment-using-streamlit)  
- [Installation & Usage](#installation--usage)  
- [Visualizations & Insights](#visualizations--insights)  
- [Future Improvements](#future-improvements)  
- [Conclusion](#conclusion)  

---

## ðŸ“‚ Dataset Overview  
**Dataset:** *Amazon Fine Food Reviews*  

### **ðŸ“Œ Context**  
This dataset consists of **Amazon fine food reviews**, covering a period of **10+ years** (1999 - 2012). The dataset includes **product and user information, review scores, and plain text reviews**. It also contains reviews from **all Amazon categories**.  

### **ðŸ“Œ Data Summary**  
âœ… **Reviews from:** *October 1999 - October 2012*  
âœ… **Total Reviews:** *568,454*  
âœ… **Total Users:** *256,059*  
âœ… **Total Products:** *74,258*  

---

### **ðŸ“Œ Features:**  
| Feature | Description |
|---------|------------|
| **Id** | Unique identifier for each review |
| **ProductId** | Unique identifier for the product |
| **UserId** | Unique identifier for the reviewer |
| **ProfileName** | Name of the reviewer |
| **HelpfulnessNumerator** | Number of helpful votes |
| **HelpfulnessDenominator** | Total votes for helpfulness |
| **Score** | Rating (1 to 5) |
| **Summary** | Short review title |
| **Text** | Full review content |

### **ðŸ“Œ Sentiment Labels:**  
- **Positive (Score > 3)**  
- **Negative (Score â‰¤ 3)**  

---

## ðŸ”Ž Exploratory Data Analysis (EDA)  

### **ðŸ”¹ Steps Performed:**  
âœ… Checking for missing values  
âœ… Analyzing sentiment distribution  
âœ… Finding the most commonly used words  

### ðŸ“ˆ **Visuals & Code**  

#### **1ï¸âƒ£ Distribution of Review Scores**  
```python
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8,5))
sns.countplot(x=df['Score'], palette="coolwarm")
plt.title("Distribution of Review Scores")
plt.xlabel("Score (1-5)")
plt.ylabel("Count")
plt.show()
```
ðŸ“Œ Insight: More positive reviews than negative ones.

## ðŸ“Š Exploratory Data Analysis (EDA)

### **2ï¸âƒ£ Most Frequent Words in Reviews**
```python
from wordcloud import WordCloud
import matplotlib.pyplot as plt

text = " ".join(review for review in df['Text'])
wordcloud = WordCloud(width=800, height=400, background_color="white").generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```
ðŸ“Œ Insight: Common words include â€œgoodâ€, â€œgreatâ€, â€œloveâ€, â€œbadâ€, â€œterribleâ€.

## ðŸ›  Data Preprocessing  

### **ðŸ”¹ Steps Taken:**  
âœ… Removing Stopwords & Punctuation  
âœ… Tokenization & Lemmatization  
âœ… TF-IDF Vectorization for Feature Extraction  
âœ… Convert Scores into Binary Sentiment Labels  

---

### **ðŸ”¹ Converting Scores into Sentiment Labels**
```python
df["Sentiment"] = df["Score"].apply(lambda x: 1 if x > 3 else 0)  # 4-5 â†’ Positive, 1-2 â†’ Negative
```

## ðŸ”¹ TF-IDF Vectorization  

To convert the text reviews into numerical representations, **TF-IDF (Term Frequency-Inverse Document Frequency)** was used. This helps in understanding the importance of words in the dataset.

### **ðŸ”¹ Implementing TF-IDF Vectorization**
```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features=5000)  # Keep top 5000 words
X_tfidf = vectorizer.fit_transform(df["Text"])
```

## ðŸ¤– Machine Learning Models  

In this project, multiple **Machine Learning models** were implemented to classify customer reviews as **Positive or Negative**.

### **ðŸ”¹ Implemented Models:**  
âœ… **Logistic Regression** - A simple and efficient classification model for text data.  
âœ… **Support Vector Machine (SVM)** - Works well for text classification, particularly in high-dimensional spaces.  
âœ… **Decision Tree Classifier** - A rule-based approach to classify sentiments, but prone to overfitting.  
âœ… **Random Forest Classifier** - An ensemble learning method that reduces overfitting and improves accuracy.  
âœ… **XGBoost** - A powerful gradient boosting algorithm that outperformed other models.  
âœ… **Stochastic Gradient Descent (SGD)** - Fast and efficient for large-scale text classification.  

ðŸ“Œ **Each model was trained, evaluated, and compared based on accuracy, precision, recall, and F1-score.**  
**XGBoost achieved the highest accuracy (90%)**, making it the best-performing model in this project. ðŸš€  

## ðŸ”¹ Training Machine Learning Models  

After preprocessing the dataset and converting text reviews into **TF-IDF features**, multiple **Machine Learning models** were trained to classify sentiments.

### **ðŸ”¹ Splitting the Dataset**  
The dataset was split into **80% training** and **20% testing** to evaluate model performance effectively.

```python
from sklearn.model_selection import train_test_split

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df["Sentiment"], test_size=0.2, random_state=42)

# Define models
models = {
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(kernel="linear"),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(n_estimators=100),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
    "SGD Classifier": SGDClassifier(loss="hinge"),
}

# Train models
for name, model in models.items():
    model.fit(X_train, y_train)
```
